---
title: "Aula 6 - Decomposição STL e Modelos de Suavização Exponencial"
subtitle: 
author: "Renato Rodrigues Silva"
institute: "Universidade Federal de Goiás."
date: "(updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

    
---
class: middle
##Decomposição STL (Cleveland et. al. 1990)

- STL é a sigla em inglês para "Seasonal Trend decomposition using Loess", que em português significa Decomposição de tendência e sazonalidade usando regressão Loess".

- A ideia principal é decompor uma série temporal em três componentes: sazonal, tendência e resíduo, para todos os pontos mensurados. Isso é feito por meio de dois "loops" aninhados. 

- Esse algoritmo foi desenvolvido por Cleveland et al. (1990), e pode ser encontrado nesse [endereço](https://www.wessa.net/download/stl.pdf) ou nesse [blog](http://www.gardner.fyi/blog/STL-Part-II/).

- A variação temporal considerada deve ser  Sazonal, Tendência ou Residual. E a atribuição desses componentes é até um certo ponto, questão de opinião e determinada pela escolha do modelo e dos parâmetros do modelo (Hyndman).


---
class: middle
##Decomposição STL: Visão Geral (Cleveland et. al. 1990)

####Modelo de decomposição

$$Y_{\nu} = T_{\nu} + S_{\nu} + R_{\nu},$$
em que $Y_{\nu}$ é o valor da série, $T_{\nu}$ é o componente de tendência, $S_{\nu}$ é o componente sazonal e $R_{\nu}$ é o erro aleatório, para $\nu = 1, \ldots, N.$ 

####Notação

- $d:$ grau do polinômio local;
- $q:$ número de pontos considerados dentro da "janela" do método lowess;
- $k:$ contador do loop interno;
- $n_{(p)}:$ o número de observações em cada ciclo de componentes sazonais;
- $n_{(i)}:$ o número de passagens pelo loop interno;
- $n_{(o)}:$ o número de iterações do loop externo;
- $n_{(l)}:$ o parâmetro de suavização para o ["filtro passa baixo"](https://pt.wikipedia.org/wiki/Filtro_passa-baixo#:~:text=Filtro%20passa%2Dbaixo%20%C3%A9%20o,varia%20de%20filtro%20para%20filtro.); 
- $n_{(t)}:$ o parâmetro de suavização para o componente de tendência;
- $n_{(s)}:$ o parâmetro de suavização para o componente sazonal.

---
class: middle
##Decomposição STL: Visão Geral (Cleveland et. al. 1990)

- STL consiste em dois procedimentos recursivos. Um loop interno aninhado dentro de um loop externo.

- Em cada passagem do loop interno os componentes de sazonalidade e tendência são atualizados uma vez.

- Cada rodada completa do loop interno consiste em $n_{(i)}$ tais passagens.

- Cada passagem do loop externo consiste de um loop interno seguido pela computação do peso de robustez.

- Estes pesos são usados na próxima rodada do loop interno para reduzir a influência de comportamento, [transiente](https://pt.wikipedia.org/wiki/Transiente#:~:text=Transiente%2C%20em%20engenharia%20el%C3%A9trica%2C%20%C3%A9,pr%C3%B3prio%20circuito%20eletr%C3%B4nico%20ao%20chaveamento.), aberrante nos componentes de tendência e sazonalidade.

---
class: middle
##Decomposição STL: Visão Geral (Cleveland et. al. 1990)

- Suponha o número de observações em cada período, ou ciclo, de componente sazonal é $n_{(p)}$.

- Por exemplo, se a série é mensal com periodicidade anual, então $n_{(p)} = 12.$ 

- Nós precisamos ser capaz de referir as subséries de valores em cada posição do ciclo sazonal.

- Por exemplo, para uma série mensal com $n_{(p)} = 12,$  a primeira subsérie são os valores de janeiro, a segunda são valores de fevereiro, e assim por diante.

- Nós vamos nos referir a essas subséries como subsérie de ciclo.

---
class: middle
##Loop interno (Cleveland et. al. 1990)

- Cada  passagem do loop interno consiste de uma suavização sazonal que atualiza o componente sazonal, seguido por uma suavização da tendência que atualiza o componente de tendência.

- Suponha $S_{\nu}^k$ e $T_{\nu}^k$ para $\nu = 1, \ldots, N$, sejam os componentes de tendência e de sazonalidade no final da $k$-ésima passagem.

- Esses dois componentes são definidos o tempo todo, $\nu = 1, \ldots, N$, mesmo no momento em que $Y_{\nu}$ está ausente.

- A atualização do $(k+1)$-ésimo passo, $S_{\nu}^{(k+1)}$ e $T_{\nu}^{(k+1)}$ são computadas da seguinte forma:


---
class: middle
##Loop interno (Cleveland et. al. 1990)

####Passo 1 - Remover a Tendência

- Uma série livre de tendência $Y_{\nu} - T_{\nu}^{(k)}$  é computada. 

- Se $Y_{\nu}$ é faltante em um particular instante de tempo, então série livre de tendência também tem dado faltante nessa posição.

---
class: middle
##Loop interno (Cleveland et. al. 1990)

####Passo 2 - Suavização de Subséries de ciclos. 

- Cada subsérie de ciclo de uma série livre de tendência é suavizada por uma regressão loess com $q = n_{(s)}$ e $d = 1$.

- Valores suavizados são computados para todos os instantes de tempo incluindo aqueles em que a série é faltante. Além disso, tem-se computados valores em um passo anterior ao começo da série
e um posterior ao último valor da série.

- Por exemplo, em uma subsérie de ciclo obtida a partir de Janeiro de 1943  a Janeiro de 1985. Os valores suavizados formam uma subsérie de  Janeiro de 1942  a Janeiro de 1986.

- A coleção de valores suavizados para todas as subséries de ciclos é uma série sazonal temporária $C_{\nu}^{(k+1)}$, que consiste de $N  + 2 n_{(p)}$ que variam de  $\nu = - n_{(p)} + 1$ até $N + n_{(p)}.$ 


---
class: middle
##Loop interno (Cleveland et. al. 1990)

####Passo 3 - Filtro passa baixo de valores suavizados das subséries de ciclos.

- Um filtro passa baixo é aplicado a $C_{\nu}^{(k+1)}.$

- O filtro consiste de uma média móvel de tamanho $n_{(p)}$, seguido por uma nova média móvel de tamanho $n_{(p)}$, seguid por uma média móvel de tamanho 3, seguido por um ajuste de uma regressão loess com $d=1$
e $q = n_{(i)}.$

- O valor de saída $L_{\nu}^{(k+1)}$ é definido em todas posições $\nu = 1, \ldots, N$ porque a média móvel com 3 elementos não pode ser estender ao final.

- $n_{(p)}$ posições são perdidas para cada fim. 


---
class: middle

####Passo 4 - Retirando a tendência das subséries de ciclo suavizadas

O componente sazonal é calculado a partir do $(k+1)$ ésimo
loop é dado por: 

$$S_{\nu}^{(k+1)} = C_{\nu}^{(k+1)} - L_{\nu}^{(k+1)},$$
para $\nu = 1, \ldots, N.$

$L_{\nu}^{(k+1)}$ é subtraído para impedir que a energia de baixa frequência entre no componente sazonal

####Passo 5 - Obtendo série livre de sazonalidade

- Uma série livre de sazonalidade $Y_{\nu} - S_{\nu}^{(k)}$ é computada.

- Se $Y_{\nu}$ é dado faltante, então a série livre de sazonalidade também é.

---
class: middle

####Passo 6 - Suavizando a tendência

- A série livre de sazonalidade é suavizada por loess com $q = n_{(l)}$
e $d =  1$. 

- Valores suavizados são computados para todos os instantes de tempo,

- O componente de tendência a partir do $(k+1)$-ésimo loop, $T_{\nu}^{(k+1)}$ para $\nu = 1, \ldots, N$ é este conjunto de valores suavizados.


---
class: middle
##Decomposição STL: loop externo (Cleveland et. al. 1990)

####Obtenção do resíduo 

$$R_{\nu} = Y_{\nu} - T_{\nu} - S_{\nu}.$$

####Definição de Peso de Robustez

- Definiremos um peso para cada ponto em que $Y_{\nu}$ é observado.

- Estes pesos de robustez reflete o quão extremos $R_{\nu}$ são.

- Um valor atípico resultará em um valor alto de $|R_{\nu}|$ e baixo
de peso.

- Seja $h = 6 \mbox{median}(|R_{\nu}|).$ Então o peso de robustez no ponto $\nu$ é $\rho_i = B\left(\frac{R_{\nu}}{h}\right)$, em que $B$ é uma função bi quadrática, ou seja

\begin{align}
B_{(u)} = \left\{\begin{array}{cc} 
(1 - |u|^2)^2,   & \mbox{se} 0 \leq |u| \leq 1 \\
0,               & c.c.   
\end{array}\right.
\end{align}
  
  

---
class: middle
##Função stl - Principais Argumentos:

* `x` - objeto da classe `ts`

* `s.window` seria equivalente ao $n_{(s)}$, ou seja, o parâmetro de suavização para o componente sazonal. De acordo com Cleveland et al. 1990, esse valor deve ser ímpar e maior ou igual a 7. Não tem default.

* `s.degree` o grau do polinômio local para extrair a sazonalidade, deve ser 0 ou 1.

* `t.window`, $n_{(t)}:$ o parâmetro de suavização para o componente de tendência; deve ser um valor ímpar. Tem default.

* `t.degree` o grau de polinômio local para extrair a tendência, deve ser 0 ou 1

* `l.window`, a extensão (em defasagens) da janela loess do filtro passa-baixo usado para cada sub-série. Deve ser ímpar, tem default.

---
class: middle
##Função stl  - Principais Argumentos


* `l.degree` grau do polinômio para o filtro passa baixo das subséries. Obrigatoriamente é 0 ou 1.

* `robust` variável lógica que indica se o ajuste robusto deve ser feito.

* `inner` o número de loop internos que o algoritmo deve executar,
normalmente utiliza-se um número baixo, por exemplo, 2.

* `outer`, o número de loop externos que o algoritmo deve executa

---
class: middle
##Exemplo no R - M-IPI

- Vamos analisar a série de Imposto de Produção Física Industrial - Produtos Alimentares, observações mensais de janeiro de 1985 a julho de 2000. 

- Maiores informações a respeito da pesquisa, [procure aqui](https://www.ibge.gov.br/estatisticas/economicas/industria/9294-pesquisa-industrial-mensal-producao-fisica-brasil.html?=&t=o-que-e) ou [aqui](https://sidra.ibge.gov.br/tabela/3653.)




```{r, echo = FALSE, warning=FALSE, message=FALSE}

library(tidyverse)
library(ggpmisc)
library(knitr)
library(kableExtra)
library(lubridate)
library(httr)
library(xlsx)
library(fpp2)
library(forecast)
library(tsibble)
library(TTR)
library(forecast)
```

---
class: middle
##Exemplo no R - Plotar a série


```{r, echo = FALSE, warning=FALSE, message=FALSE}

url1 = 'https://www.ime.usp.br/~pam/IPI.XLS'
a = GET(url1, write_disk(tf <- tempfile(fileext = ".xls")))
dat =  as_tibble(read.xlsx(tf, sheetIndex = 1))

IPI = ts(dat$ipialiment, start = c(1985,1), frequency=12)
autoplot(IPI) + theme_bw()

```

---
class: middle
##Exemplo no R - Plotar subseries por ano

```{r, echo = FALSE, warning=FALSE, message=FALSE}
ggseasonplot(IPI, year.labels=TRUE, year.labels.left=TRUE ) + theme_bw()
``` 

---
class: middle
##Exemplo no R - Plotar subseries de ciclo

```{r, echo = FALSE, warning=FALSE, message=FALSE}
dat = mutate(dat, mes = month(dat$mês.ano), ano = year(dat$mês.ano))

ggplot(dat,aes(x = ano, y = ipialiment)) + geom_boxplot() + facet_grid(. ~ mes, scales = "free")+xlab('meses') +  ylab('indice')+ theme(panel.border=element_blank(), axis.line=element_line(),            axis.text.x=element_blank())

```


---
class: middle
###Exemplo no R - Decomposição STL

```{r, echo = FALSE, warning=FALSE, message=FALSE}
IPI %>%
stl(t.window=15, s.window=31, robust=FALSE, inner = 2, s.degree=1,l.degree = 1,t.degree=1) %>%
  autoplot()
```

[explicação interessante](https://stats.stackexchange.com/questions/7876/interpreting-range-bars-in-rs-plot-stl)

---
class: middle, inverse, center
#Métodos de suavização


---
class: middle, 

##Revisão - Aulas passadas

####Tendência: (Rob Hyndman)

-  Uma mudança a longo prazo no nível da série. Crescimento ou descrescimento [curso](https://github.com/robjhyndman/ETC3550Slides/raw/fable/2-tsgraphics.pdf). 


Modelo de decomposição de séries temporal sem levar em consideração o componente sazonal.

$$Z_t = T_t + a_t.$$

####Métodos para Estimar Tendência: (Morretin & Toloi 2004)

- Ajuste de uma função no tempo (polinômios, exponenciais e etc);

- Filtrar os valores da série ao redor de um ponto (médias móveis centradas);

- Suavisar os valores da série através de um ajuste de regressão loess.

---
class: middle, 
##Métodos de Suavização - Introdução (Morettin & Toloi 2004)

- A maioria dos métodos de previsão baseia-se na ideia de que observações passadas contêm informações sobre o padrão de comportamento da série temporal.

- O propósito dos métodos é distinguir o padrão de qualquer ruído que possa estar contido nas observações e então usar esse padrão para prever valores futuros da série.

- Uma grande classe de métodos de previsão, que tenta tratar ambas as causas de flutuações em séries de tempo, é das suavizações.

- Técnicas específicas desse tipo assumem que os valores de extremos da série representam a aleatoriedade e, assim, por meio da suavização desses extremos, pode-se identificar o padrão básico. 


---
class: middle

##Modelos para séries localmente constantes 

- Vamos considerar, nesta seção, o caso de uma série temporal $Z_1, \ldots, Z_N,$ localmente composta de seu nível mais um ruído aleatório, isto é,

$$Z_t = \mu_t + a_t, \phantom{11} t = 1, \ldots, N,$$
em que $E[a_t] = 0$, $Var[a_t] = \sigma^2_a$ e $\mu_t$ é um parâmetro desconhecido, que pode variar lentamente com o tempo.

- [Outras explicações](http://www.math.hkbu.edu.hk/~hpeng/Math4826/Chapter3.pdf)

- [Outras explicações](http://www.imm.dtu.dk/~hmad/time.series.analysis/slides/lect03.pdf)

---
class: middle

##Média móveis simples

$$M_t = \frac{Z_t + Z_{t-1} + \ldots + Z_{t-r+1}}{r} = \frac{1}{r}\sum_{k=0}^{r-1} Z_{t-k} ,$$
#### Forma recursiva

- Usando a expressão anterior temos:

$$M_{t-1} = \frac{Z_{t-1} + Z_{t-2} + \ldots + Z_{t-r}}{r}$$

 - Logo,
 
\begin{align}
M_t =& \frac{Z_t + Z_{t-1} + \ldots + Z_{t-r+1}}{r} \\
=& \frac{Z_t}{r} + \frac{Z_{t-1} + Z_{t-2} + \ldots + Z_{t-r+1}}{r} +\frac{Z_{t-r}}{r} -  \frac{Z_{t-r}}{r} \\
    =& \frac{Z_t- Z_{t-r}}{r} + \frac{Z_{t-1} + Z_{t-2} + \ldots + Z_{t-r}}{r} \\
    =&  \frac{Z_t - Z_{t-r}}{r} + M_{t-1}.
\end{align}


---
class: middle
##Previsão

- A previsão de todos os valores futuros é dada pela última média móvel calculada, isto é, 

$$\hat{Z}_{t+h} =  M_t = \frac{1}{r}\sum_{k=0}^{r-1}\mu_{t-k}, \phantom{111} \forall \phantom{|} h > 0.$$

ou

$$\hat{Z}_{t+h}  = \hat{Z}_{t+h-1}  + \frac{Z_t - Z_{t-r}}{r}, \phantom{11} h > 0.$$ 

- Essa equação pode ser interpretada como um mecanismo de atualização de previsão, pois a cada instante corrige a estimativa prévia de $\hat{Z}_{t+h}.$

####Média de Previsão 

$$E[\hat{Z}_{t+h} ] = E\left(\frac{1}{r}\sum_{k=0}^{r-1} Z_{t-k}\right) = 
\frac{1}{r}\sum_{k=0}^{r-1} \mu_{t-k}.$$



---
class: middle
##Erro quadrático médio de previsão

\begin{align}
\mbox{EQM}[\hat{Z}_{t+h} ] =& E\left[Z_{t+h} - \hat{Z}_{t+h}\right]^2 \\
=& E\left[ Z_{t+h} - \sum_{k=0}^{r-1} \frac{Z_{t-k}}{r}\right]^2 \\
=& E\left[ Z_{t} - \sum_{k=0}^{r-h-1} \frac{Z_{t-h-k}}{r}\right]^2 \\
=& E[Z_t^2] - \frac{2}{r} \sum_{k=0}^{r-h-1} E[Z_t Z_{t-h-k}] +\frac{1}{r^2}\sum_{k=0}^{r-h-1}\sum_{j=0}^{r-h-1}E[Z_{t-j-k} Z_{t-h-k}] \\
=& \gamma_t(0) - \frac{2}{r}\sum_{k=0}^{r-h-1} \gamma_t(h + k) +
\frac{1}{r^2} \sum_{k=0}^{r-h-1} \sum_{j=0}^{r-h-1}  \gamma_{t-h-k}(j - k).
\end{align}
em que $\gamma_t(h)  = E[Z_t Z_{t-h}] = Cov(Z_t,Z_{t-h-k}) + \mu_t\mu_{t-h-k}, \phantom{111} \forall \phantom{|} h > 0.$

---
class: middle

###Caso particular:  Modelo com média globalmente constante

$$Z_t = \mu + a_t,$$

- Nesse caso, temos:

$$E[\hat{Z}_{t+h}] = \frac{1}{r}\sum_{k=0}^{r-1}\mu = \mu$$
e  variância é dada por:

$$Var[\hat{Z}_{t+h}] = Var\left(\sum_{k=0}^{r-1} \frac{Z_{t-k}}{r}\right) = \frac{\sigma_a^2}{r}.$$

##Intervalo de Confiança

- Assumindo $a_t \sim N(0, \sigma^2_{a})$, pode-se afirmar que $\hat{Z}_{t+h} \sim N(\mu, \frac{\sigma^2_{a}}{r})$, podemos construir um intervalo de confiança dado por:

$$\left(\hat{Z}_{t+h} - z_{tab} \frac{\sigma_{a}}{\sqrt{r}}; \hat{Z}_{t+h}  + z_{tab} \frac{\sigma_{a}}{\sqrt{r}}\right).$$

---
class: middle
##Determinação de $r$

- As propriedades do método dependem do número de observações utilizadas na média (valor de $r$).

- Um valor grande de $r$ faz com que a previsão acompanhe lentamente as mudanças do parâmetro $\mu_t$.

- Um valor pequeno implica numa reação mais rápida.

####Casos extremos:

- $r=1$, então o valor mais recente da série é utilizado como previsão de todos os valores futuros ("método ingênuo");

- $r=N$, então a previsão será igual a média aritmética de todos os dados observados. Este caso só é indicado quando a série é altamente aleatória.


---
class: middle
##Procedimento objetivo

- Um procedimento objetivo é selecionar o valor de $r$ que fornece a "melhor previsão" a um passo das observações já obtidas ("backforecasting"), ou seja, 

- Encontrar o valor de $r$ que minimize 

$$S = \sum_{t=l+1}^N(Z_t - \hat{Z}_{t-1}(1))^2,$$
em que $l$ é escolhido de tal modo que o valor inicial utilizado em $M_t = \frac{Z_t - Z_{t-r}}{r} + M_{t-1}$ não influencie a previsão.

---
class: middle
##Vantagens e desvantagens do método (Morettin)

###Vantagens

- simples aplicação;

- é aplicável quando se tem um número pequeno de observações;

- permite uma flexibilidade grande devido à variação de $r$ de acordo com o padrão da série.

###Desvantagens

- necessidade de armazenar pelo menos $(r-1)$ observações;

- dificuldade em determinar o valor de $r$.

###Obs:

- Na prática, o método de móveis não é utilizado frequentemente.

---
class: middle
##Métodos de médias móveis - Aplicação

- Vamos aplicar o método de médias móveis à série $A_6$ CO, no período de primeiro de janeiro a 30 de abril de 1997.

- Vamos ajustar esse modelo utilizando $r=7, r=14$ e $r=21.$

- Para cada ajuste vamos calcular o erro quadrático médio.

---
class: middle
##Métodos de médias móveis - Aplicação



```{r, echo = FALSE, warning=FALSE, message=FALSE}

url1 = 'https://www.ime.usp.br/~pam/poluicao.xls'
a = GET(url1, write_disk(tf <- tempfile(fileext = ".xls")))
dat =  as_tibble(read.xlsx(tf, sheetIndex = 1))
datCO = ts(dat$co[1:120], c(1997,1),c(1997,120), frequency=365)


plot.ts(datCO)
```

---
class: middle
##Métodos de médias móveis - Aplicação


```{r, echo = FALSE, warning=FALSE, message=FALSE}

n = 120

dat2 = tibble(data = rep(1:120,4),
             co = c(datCO,
                         SMA(datCO, 7),
                         SMA(datCO, 14),
                         SMA(datCO, 21)
                         ),
              obs_estimadores = c(rep("CO",n),
                        rep("mm7",n),
                        rep("mm14",n),
                        rep("mm21",n)
                        )
)
ggplot(dat2, aes(data, co)) + 
  #geom_point(position=position_jitter(1,3), pch=21, fill="#FF0000AA") +
  geom_line(aes(y=co,col= obs_estimadores )) +  theme_bw() + xlab('Data (Diaria)') + 
  ylab('CO')
  
  


```


---
class: middle
####Métodos de médias móveis - Aplicação


```{r, echo = FALSE, warning=FALSE, message=FALSE}


data.frame(dados = datCO,
           mm7 = SMA(datCO, 7),
           mm14 = SMA(datCO, 14),
           mm21 = SMA(datCO, 21))[106:120,]

```                       
                         
####Valores de EQM 

- $r=7$, (`r  mean(c(datCO-SMA(datCO, 7))^2,na.rm=T)`);

- $r=14$, (`r  mean(c(datCO-SMA(datCO, 14))^2,na.rm=T)`);

- $r=21$, (`r  mean(c(datCO-SMA(datCO, 21))^2,na.rm=T)`);



---
class: middle
##Métodos de suavização exponencial simples (SES)

###Motivação 

- O método de médias móveis considera todas as observações (recentes e mais remotas) com o mesmo peso. 

###Definição do método

####Suavizador Ponderado exponencialmente

$$\sum_{k=0}^{N-1} \theta^k Z_{N-k} = Z_N+ \theta Z_{N-1} + \theta^2 Z_{N-2} + \ldots +
\theta^{N-1} Z_1.$$
em que $-1 < \theta < 1$ [progressão geométrica](https://www.mathsisfun.com/algebra/sequences-sums-geometric.html).

- Perceba que nesse suavizador, observações recentes tem um peso maior que observações passadas.

- Perceba também que a soma dos pesos é uma soma de progressão geométrica
$\sum_{k=0}^{N-1} \theta^k = \frac{1 - \theta^N}{1 - \theta}$ que não necessariamente soma 1.


---
class: middle
##Métodos de suavização exponencial simples (SES)

- Para resolver esse problema, multiplicamos $\sum_{k=0}^{N-1} \theta^k$ por $\frac{1 - \theta}{1 - \theta^N}$. No entanto, para valores de $N$ suficientemente grandes, $\theta^N \rightarrow 0$, logo $\frac{1 - \theta}{1 - \theta^N} \rightarrow (1 - \theta).$

- Assim temos

\begin{align}
\bar{Z}_N =& (1 - \theta) \sum_{t=0}^{N-1} \theta^t Z_{N-t} \\
=& (1 - \theta)(Z_N + \theta Z_{N-1} + \theta^2 Z_{N-2} + \ldots + \theta ^{N-1}Z_1) \\
=& (1 - \theta)Z_N + \theta (1 - \theta)(Z_{N-1} + \theta Z_{N-2} + \ldots + \theta ^{N-2}Z_1) \\
=& (1 - \theta)Z_N +  \theta \bar{Z}_{N-1}.
\end{align}

 - Fazendo $\alpha = (1 - \theta)$ e generalizando para um $t=1,\ldots, N$, temos $\bar{Z}_t = \alpha Z_t +  (1 - \alpha) \bar{Z}_{t-1},$ em que $\bar{Z}_0 = Z_1.$

---
class: middle
##Métodos de suavização exponencial simples (SES)

###Procedimento

$$\bar{Z}_t = \alpha Z_t + (1 - \alpha) \bar{Z}_{t-1}, \phantom{111} \bar{Z_0} = Z_1, \phantom{111} t = 1, \ldots, N.$$
ou

$$\bar{Z}_t = \alpha \sum_{k=0}^{t-1} (1 - \alpha)^k  Z_{t-k} + (1-\alpha)^t\bar{Z}_0,  \phantom{111} t = 1, \ldots, N.$$

- Observe que  $\bar{Z}_1 = \alpha Z_1 + (1 - \alpha) \bar{Z}_{0} \Rightarrow  \alpha Z_1 + (1 - \alpha) Z_1 = Z_1$,  portanto, $\bar{Z_0} = \bar{Z_1} = Z_1.$ [somatório](https://math.stackexchange.com/questions/35080/upper-limit-of-summation-index-lower-than-lower-limit), [somatório2](https://pt.wikipedia.org/wiki/Somat%C3%B3rio).



###Efetuando a expressão anterior temos

$$\bar{Z}_t = \alpha Z_t + \alpha(1 - \alpha) Z_{t-1} + \alpha (1  - \alpha)^2 Z_{t - 2} + \ldots + \alpha(1 - \alpha)^{t-1} Z_{1} + (1 - \alpha)^{t}\bar{Z}_0,$$
o que significa que a SES é uma média ponderada que dá pesos maiores às observações mais recentes, o que é uma vantagem ao método de médias móveis.


---
##Previsão

$$\hat{Z}_{t+h}  = \bar{Z}_t, \phantom{111} \forall h > 0,$$

$$\hat{Z}_{t+h} =  \alpha Z_t + (1 - \alpha) \hat{Z}_{t+h-1},$$

Pode-se provar que para $h=1$, tem-se

$$\hat{Z}_{t+1} = \alpha e_t + \hat{Z}_t,$$
em que $e_t = Z_t - \hat{Z}_{t}$.


####Demonstração:

\begin{align}
\hat{Z}_{t+1} &= \alpha Z_t + (1 - \alpha) \hat{Z}_{t} \\
&= \alpha Z_t - \alpha \hat{Z}_{t} + \hat{Z}_{t} \\
&= \alpha (Z_t - \hat{Z}_{t}) + \hat{Z}_{t} \\
&= \alpha e_t + \hat{Z}_{t}
\end{align}

- Assim, a nova previsão pode ser obtida da anterior, adicionando-se um múltiplo do erro de previsão, indicando que a previsão está sempre alerta a mudanças no nível de série, revelada pelo erro de previsão.

---
##Previsão - Propriedades

####Valor Esperado

\begin{align}
E[\hat{Z}_{t+h}] =& E[\alpha \sum_{k=0}^{t-1} (1 - \alpha)^k  Z_{t-k} + (1-\alpha)^t\bar{Z}_0] \\
=& \alpha \sum_{k=0}^{t-1} (1 - \alpha)^k E[Z_{t-k}] + (1-\alpha)^t E[\bar{Z}_0] \approx&  \alpha \sum_{k=0}^{t-1} (1 - \alpha)^k \mu_{t-k}
\end{align}

####Erro Quadrático Médio da Previsão

\begin{align}
\mbox{EQM}[\hat{Z}_{t+h} ] =& E\left[Z_{t+h} - \hat{Z}_{t+h}\right]^2 = E\left[ Z_{t+h} -  \alpha\sum_{k=0}^{t-1} (1 - \alpha)^k Z_{t-k} \right]^2 \\
=& E\left[ Z_{t} - \alpha\sum_{k=0}^{t-h-1} (1 - \alpha)^k Z_{t-h-k}\right]^2 \\
=& E[Z_t^2] - 2\alpha \sum_{k=0}^{t-h-1}(1-\alpha)^k E[Z_t Z_{t-h-k}] +\alpha^2\sum_{k=0}^{t-h-1}\sum_{j=0}^{t-h-1}(1 - \alpha)^{k+j} E[Z_{t-j-k} Z_{t-h-k}] \\
=& \gamma_t(0) - 2\alpha\sum_{k=0}^{t-h-1}(1-\alpha)^k \gamma_t(h + k) +
\alpha^2\sum_{k=0}^{t-h-1}\sum_{j=0}^{t-h-1}(1 - \alpha)^{k+j}  \gamma_{t-h-k}(j - k).
\end{align}


---
class: middle
###Determinação da constante $\alpha$ (Morettin)

- Quanto menor for o valor de $\alpha$ mais estáveis serão as previsões finais, uma vez que a utilização de baixo valor de $\alpha$ implica que pesos maiores serão dados as observações.

- Consequentemente, qualquer flutuação aleatória, no presente, exercerá um peso menor no cálculo da previsão.

- Em geral, quanto mais aleatória for a série  estudada, menores serão os valores da constante de suavização.

- O efeito de $\alpha$ grande ou pequeno é completamente análogo (em direção oposta) ao efeito do parâmetro $r$ no método de MMS.

- Um procedimento mais objetivo é selecionar o valor que fornece a "melhor previsão" das observações já obtidas.

---
class: middle

###Vantagens da SES

- fácil de entendimento;

- grande flexibildiade permitida pela variação da constante de suavização 
- necessidade de armazenar somente $Z_t$, $\bar{Z}_t$ e $\alpha$

###Desvantagens da SES

- A principal desvantagem é a dificuldade em determinar o valor mais apropriado da constante de suavização.


---
class: middle
##Métodos suavização exponencial simples - Aplicação

- Vamos aplicar o método de médias móveis à série $A_6$ $NO_2$, no período de primeiro de janeiro a 30 de abril de 1997.

- Vamos ajustar esse modelo utilizando $\alpha = 0.5502$. 

---
class: middle
##Métodos suavização exponencial simples - Aplicação


```{r, echo = FALSE, warning=FALSE, message=FALSE}

datNO2 = ts(dat$no2[1:120], c(1997,1),c(1997,120), frequency=365)


plot.ts(datNO2)
```

---
class: middle
##Métodos suavização exponencial simples - Aplicação


```{r, echo = FALSE, warning=FALSE, message=FALSE}

alpha  = 0.5502

SES = HoltWinters(datNO2, beta=FALSE, gamma=FALSE, alpha=0.5502)

dat2 = tibble(data = rep(1:120,2),
             no2 = c(datNO2,
                     c(NA,fitted(SES)[,1])
                         ),
              obs_estimadores = c(rep("NO2",n),
                        rep("SES",n)
                        )
)
ggplot(dat2, aes(data, no2)) + 
  #geom_point(position=position_jitter(1,3), pch=21, fill="#FF0000AA") +
  geom_line(aes(y=no2,col= obs_estimadores )) +  theme_bw() + xlab('Data (Diaria)') + 
  ylab('NO2')
  


```


---
class: middle
##Métodos suavização exponencial simples  - Aplicação


```{r, echo = FALSE, warning=FALSE, message=FALSE}


forecast(SES, h=5)

```                       
                         
[Explicação da previsão](https://otexts.com/fpp2/ses.html)


---
class: middle
#Método de suavização exponencial de Holt

- Holt (1957) estendeu a suavização exponencial simples para permitir a previsão de dados com uma tendência. 

- Esse método envolve uma equação de previsão e duas equações de suavização (uma para o nível e outra para a tendência)

- Os valores do nível e da tendência da série, no instante t, serão estimados por:

\begin{align}
\bar{Z}_t =& AZ_t + (1-A)(\bar{Z}_{t-1} +\hat{T}_{t-1}), \phantom{11} 0 < A < 1, \phantom{11} \mbox{e} \phantom{11} t = 2, \ldots, N, \\
\bar{T}_t =& C(\bar{Z}_t - \bar{Z}_{t-1}) + (1-C)\hat{T}_{t-1}, \phantom{11} 0 < C < 1, \phantom{11} \mbox{e} \phantom{11} t = 2, \ldots, N.
\end{align}
em que $A$ e $C$ são constantes de suavização.

- Essas fórmulas como em todos os métodos de suavização, modificam estimativas prévias quando uma nova observação é obtida.



---
class: middle
#Método de suavização exponencial de Holt


##Previsão

$$\hat{Z}_{t + h} = \bar{Z}_t + h \hat{T}_t, \forall h > 0,$$
ou seja, a previsão é feita adicionando-se ao valor básico. $(\bar{Z}_t)$ a tendência multiplicada pelo número de passos à frente que se deseja prever $(h)$.


###Determinação das constantes de suavização

- Escolher valores de (A,C) que minimizem o erro quadrático de previsão ("backforecasting")


---
class: middle
#Método de suavização exponencial de Holt - Aplicação

- Vamos analisar a série $A_{10}$ (M-ICV), índice de custo de vida no município de São Paulo; que originalmente possui observações mensais de janeiro de 1970 a junho de 1980.

- Por questões didáticas, vamos analisar a série no período de janeiro de 1970 (t=1) a junho de 1979 (t=114), ou seja, isolamos as 12 ultimas observações com o objetivo de comparar as previsões com os respectivos valores reais.

```{r, echo = FALSE, warning=FALSE, message=FALSE}

url1 = 'https://www.ime.usp.br/~pam/ICV.xls'
a = GET(url1, write_disk(tf <- tempfile(fileext = ".xls")))
dat =  as_tibble(read.xlsx(tf, sheetIndex = 1))
datM_ICV = ts(dat$ICV[1:114], c(1970,1), frequency=12)

```

---
class: middle
###Método de suavização exponencial de Holt - Aplicação


```{r, echo = FALSE, warning=FALSE, message=FALSE}

autoplot(datM_ICV)

```


---
class: middle
###Método de suavização exponencial de Holt - Aplicação

```{r, echo = FALSE, warning=FALSE, message=FALSE}
Holt = HoltWinters(datM_ICV, beta=0.3, gamma=FALSE, alpha=0.9)

forecast(Holt, h=12)

```

---
class: middle
###Método de suavização exponencial de Holt - Aplicação

```{r, echo = FALSE, warning=FALSE, message=FALSE}

n = 126

datResp = tibble(data = rep(1:n,3),
             dados_obs = c(
               ts(dat$ICV, c(1970,1), frequency=12),
               c(NA,NA,fitted(Holt)[,1], rep(NA,12)),
               c(rep(NA,114),predict(Holt, 12, prediction.interval=TRUE)[,1])),
              estimadores = c(rep("dados obs",n),
                        rep("HW (ajuste)",n),
                        rep("HW (previsão)",n)
                        )
)

ggplot(datResp , aes(data, dados_obs )) + 
  geom_line(aes(y=dados_obs ,col= estimadores )) +  theme_bw() + xlab('Dados (Mensal)') + ylab('IPI') + ggtitle("Suavização de Holt")
 
```


---
class: middle
#Métodos de suavização para séries sazonais

- Para séries temporais que apresentam um padrão de comportamento mais complexo, existem outras formas de suavização

- Nessa aula veremos o método de Holt-Winters.

Winters, P. R. (1960). Forecasting sales by exponentially weighted moving averages. Management Science, 6, 324–342. (https://doi.org/10.1287/mnsc.6.3.324)

---
class: middle
#Método de suavização exponencial de Holt-Winters

####Equações de Suavização - Série Sazonal Multiplicativa 

\begin{align}
\hat{F}_t =& D\left(\frac{Z_t}{\bar{Z}_t}\right) + (1 - D) \hat{F}_{t - s}, \phantom{11} 0 < D < 1, \phantom{111}, t = s + 1, \ldots, N. \\
\bar{Z}_t =& A\left(\frac{Z_t}{\hat{F}_{t-s}}\right) + (1 - A)(\bar{Z}_{t-1} + \hat{T}_{t-1}), \phantom{11} 0 < A < 1, \phantom{111}, t = s + 1, \ldots, N. \\
\hat{T}_t =& C(\bar{Z}_t - \bar{Z}_{t-1}) + (1 - C)\hat{T}_{t-1}, \phantom{111}, 0 < C < 1, \phantom{111} t = s+1, \ldots, N.
\end{align}

####Previsão - Série Sazonal Multiplicativa


$$\hat{Z}_{t+h} = (\bar{Z}_t + h\hat{T}_t)\hat{F}_{t+h-s}, \phantom{111} h = 1,2, \ldots, s$$




---
class: middle
#Método de suavização exponencial de Holt-Winters

####Equações de Suavização - Série Sazonal Aditiva

\begin{align}
\hat{F}_t =& D\left(Z_t - \bar{Z}_t\right) + (1 - D) \hat{F}_{t - s}, \phantom{11} 0 < D < 1, \phantom{11} t = s + 1, \ldots, N. \\
\bar{Z}_t =& A\left(Z_t - \hat{F}_{t-s}\right) + (1 - A)(\bar{Z}_{t-1} + \hat{T}_{t-1}), \phantom{11} 0 < A < 1, \phantom{11} t = s + 1, \ldots, N. \\
\hat{T}_t =& C(\hat{Z}_t - \bar{Z}_{t-1}) + (1 - C)\hat{T}_{t-1}, \phantom{11} 0 < C < 1, \phantom{11} t = s+1, \ldots, N.
\end{align}

####Previsão - Série Sazonal Aditiva

$$\hat{Z}_{t+h} = \bar{Z}_t + h\hat{T}_t + \hat{F}_{t+h-s}, \phantom{111} h = 1,2, \ldots, s.$$

---
class: middle
###Método de Holt Winters Série Multiplicativa - Aplicação 

$$Z_t = \mu_t F_t + T_t + a_t, \phantom{111}, t = 1, \ldots, N.$$

- Aplicamos o método HW multiplicativo a série IPI.

- É uma série periódica com $s=12$, durante o período de janeiro de 1969 a julho de 1979.

---
class: middle
###Método de Holt Winters Série Multiplicativa - Aplicação 

```{r, echo = FALSE, warning=FALSE, message=FALSE}

x = c(7.78,7.351,8.317,8.036, 8.424,8.3,8.985, 8.589, 8.564,8.614, 8.102,8.044,
      8.209, 7.738, 8.828, 9.150,8.960, 9.282, 9.934, 9.546, 9.572, 10.272, 9.991, 9.537,
      8.761, 8.501, 9.642, 9.058, 9.256, 9.799, 10.828, 11.063, 10.652, 11.278, 10.661, 10.500,
      9.759, 9.876, 10.664,10.110, 11.055,11.615, 11.730, 12.587,12.046, 12.852, 12.259, 12.214,
      11.798, 11.278, 11.945, 11.695, 12.734, 13.405, 13.836, 14.388, 14.069, 15.519, 14.680, 14.104,
      13.577, 12.451, 13.856, 13.812, 14.280, 13.692, 15.502, 15.423, 14.947, 16.031,14.462, 13.791,
      14.829, 15.297, 16.330, 15.807, 16.623, 17.196, 17.691, 18.012, 17.625, 18.244, 17.102, 16.744,
      15.385, 15.062, 17.896, 16.262, 17.820, 17.911, 17.818, 18.410, 17.658, 18.273, 17.922, 16.987,
      16.681, 15.886, 18.281, 17.478, 18.412, 18.849, 19.023, 20.372, 19.262, 20.570, 19.304, 18.407,
      18.633, 17.497, 19.470, 18.884, 20.308, 20.146, 20.258, 21.614, 19.717, 22.133, 20.503, 18.800,
      19.577, 18.992, 21.022, 19.064,21.067, 21.553, 22.513)

datM_IPI = ts(x, c(1969,1), c(1979,7),frequency=12)

```


```{r, echo = FALSE, warning=FALSE, message=FALSE}

autoplot(datM_IPI)

```


---
class: middle
###Método de Holt Winters Série Multiplicativa - Aplicação

```{r, echo = FALSE, warning=FALSE, message=FALSE}

HoltW = HoltWinters(datM_IPI, beta=0.1, gamma=0.3, alpha=0.3,
                   seasonal=c("multiplicative"))

forecast(HoltW, h=12)

```

---
class: middle
###Método de Holt Winters Série Multiplicativa - Aplicação

```{r, echo = FALSE, warning=FALSE, message=FALSE}


n = 127

datResp = tibble(data = rep(1:n,3),
             dados_obs = c(
               ts(x, c(1969,1),frequency=12), 
               c(fitted(HoltW)[,1], rep(NA,12)),
               c(rep(NA,115),predict(HoltW, 12, prediction.interval=TRUE)[,1])),
              estimadores = c(rep("dados obs",n),
                        rep("HW (ajuste)",n),
                        rep("HW (previsão)",n)
                        )
)

ggplot(datResp , aes(data, dados_obs )) + 
  geom_line(aes(y=dados_obs ,col= estimadores )) +  theme_bw() + xlab('Dados (Mensal)') + ylab('IPI') + ggtitle("Série Multiplicativa")
 


```
